#!/usr/bin/env python3
"""
Ð¡ÐºÑ€Ð¸Ð¿Ñ‚ Ð´Ð»Ñ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ¸ TikTok-10M Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð° Ðº fine-tuning VideoMAE.

Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ñ‚ binary classification Ð´Ð°Ñ‚Ð°ÑÐµÑ‚: viral vs non-viral Ð²Ð¸Ð´ÐµÐ¾
Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ engagement Ð¼ÐµÑ‚Ñ€Ð¸Ðº.
"""

import os
from datasets import load_dataset
import pandas as pd
from pathlib import Path
import json


def calculate_virality_score(row):
    """
    Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÑ‚ virality score Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ engagement Ð¼ÐµÑ‚Ñ€Ð¸Ðº.

    Formula:
        engagement_rate = (likes + shares*3 + comments*2) / views
        viral if: engagement_rate > 5% OR views > 1M

    Returns:
        dict: {"is_viral": bool, "engagement_rate": float, "virality_score": int}
    """
    views = row.get('play_count', 1)  # avoid division by zero
    likes = row.get('digg_count', 0)
    shares = row.get('share_count', 0)
    comments = row.get('comment_count', 0)

    # Weighted engagement calculation
    # Shares count 3x (most valuable signal)
    # Comments count 2x (high engagement indicator)
    # Likes count 1x (baseline engagement)
    weighted_engagement = likes + (shares * 3) + (comments * 2)
    engagement_rate = weighted_engagement / max(views, 1)

    # Virality criteria:
    # 1. High engagement rate (>5% is exceptional for TikTok)
    # 2. OR high absolute views (>1M = viral by definition)
    is_viral = (engagement_rate > 0.05) or (views > 1_000_000)

    # Score 0-100 based on combination of metrics
    # 50% from engagement rate, 50% from view count
    engagement_score = min(100, engagement_rate * 1000)  # 5% = 50 points
    views_score = min(100, (views / 2_000_000) * 100)    # 2M views = 50 points
    virality_score = int((engagement_score + views_score) / 2)

    return {
        "is_viral": is_viral,
        "engagement_rate": engagement_rate,
        "virality_score": virality_score,
        "metrics": {
            "views": views,
            "likes": likes,
            "shares": shares,
            "comments": comments
        }
    }


def filter_valid_videos(dataset, min_views=1000, max_duration=180):
    """
    Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÑ‚ Ð²Ð¸Ð´ÐµÐ¾ Ð¿Ð¾ ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸ÑÐ¼ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°.

    Args:
        dataset: HuggingFace dataset
        min_views: ÐœÐ¸Ð½Ð¸Ð¼ÑƒÐ¼ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¾Ð² (Ñ„Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ ÑÐ¿Ð°Ð¼/Ð½Ð¾Ð²Ñ‹Ðµ Ð²Ð¸Ð´ÐµÐ¾)
        max_duration: ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð² ÑÐµÐºÑƒÐ½Ð´Ð°Ñ… (Ñ„Ð¾ÐºÑƒÑ Ð½Ð° shorts)

    Returns:
        Filtered dataset
    """
    def is_valid(example):
        views = example.get('play_count', 0)
        duration = example.get('duration', 0)

        # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹:
        # 1. ÐœÐ¸Ð½Ð¸Ð¼ÑƒÐ¼ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¾Ð² (ÑƒÐ±Ð¸Ñ€Ð°ÐµÑ‚ ÑÐ¿Ð°Ð¼ Ð¸ Ð½Ð¾Ð²Ñ‹Ðµ Ð²Ð¸Ð´ÐµÐ¾ Ð±ÐµÐ· ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸)
        # 2. ÐœÐ°ÐºÑÐ¸Ð¼ÑƒÐ¼ Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ (Ñ„Ð¾ÐºÑƒÑ Ð½Ð° short-form ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚)
        # 3. Ð•ÑÑ‚ÑŒ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ (quality signal)
        has_description = bool(example.get('desc', '').strip())

        return (
            views >= min_views and
            0 < duration <= max_duration and
            has_description
        )

    return dataset.filter(is_valid)


def prepare_dataset_for_training(
    output_dir: str = "./data/tiktok_viral",
    num_samples: int = 100_000,
    viral_ratio: float = 0.3,
    test_split: float = 0.1,
    val_split: float = 0.1
):
    """
    ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÑ‚ balanced Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ Ð´Ð»Ñ training VideoMAE.

    Args:
        output_dir: Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ
        num_samples: ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð²Ð¸Ð´ÐµÐ¾ Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
        viral_ratio: ÐŸÑ€Ð¾Ñ†ÐµÐ½Ñ‚ viral Ð²Ð¸Ð´ÐµÐ¾ Ð² Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ðµ (0.3 = 30%)
        test_split: Ð Ð°Ð·Ð¼ÐµÑ€ test set (0.1 = 10%)
        val_split: Ð Ð°Ð·Ð¼ÐµÑ€ validation set (0.1 = 10%)
    """

    print(f"ðŸš€ Loading TikTok-10M dataset (first {num_samples:,} samples)...")

    # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°
    dataset = load_dataset(
        "The-data-company/TikTok-10M",
        split=f"train[:{num_samples}]",
        streaming=False  # Load into memory for faster processing
    )

    print(f"âœ… Loaded {len(dataset):,} videos")

    # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð¿Ð¾ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ñƒ
    print("ðŸ” Filtering valid videos (min 1K views, max 180s duration)...")
    dataset = filter_valid_videos(dataset, min_views=1000, max_duration=180)
    print(f"âœ… Filtered to {len(dataset):,} valid videos")

    # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ virality scores
    print("ðŸ“Š Calculating virality scores...")
    viral_videos = []
    non_viral_videos = []

    for idx, example in enumerate(dataset):
        if idx % 10000 == 0:
            print(f"  Processed {idx:,} / {len(dataset):,} videos...")

        virality_data = calculate_virality_score(example)

        video_entry = {
            "video_id": example.get('id', f'video_{idx}'),
            "duration": example.get('duration', 0),
            "description": example.get('desc', ''),
            "hashtags": example.get('challenges', []),
            "create_time": example.get('create_time', ''),
            **virality_data
        }

        if virality_data['is_viral']:
            viral_videos.append(video_entry)
        else:
            non_viral_videos.append(video_entry)

    print(f"âœ… Found {len(viral_videos):,} viral videos")
    print(f"âœ… Found {len(non_viral_videos):,} non-viral videos")

    # Ð‘Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ° Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°
    target_viral = int(num_samples * viral_ratio)
    target_non_viral = num_samples - target_viral

    print(f"\nâš–ï¸  Balancing dataset to {viral_ratio*100:.0f}% viral...")
    print(f"  Target: {target_viral:,} viral + {target_non_viral:,} non-viral")

    # Ð¡ÑÐ¼Ð¿Ð»Ð¸Ñ€ÑƒÐµÐ¼ Ð½ÑƒÐ¶Ð½Ð¾Ðµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾
    import random
    random.shuffle(viral_videos)
    random.shuffle(non_viral_videos)

    viral_sample = viral_videos[:target_viral]
    non_viral_sample = non_viral_videos[:target_non_viral]

    # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ð¸ Ð¿ÐµÑ€ÐµÐ¼ÐµÑˆÐ¸Ð²Ð°ÐµÐ¼
    all_videos = viral_sample + non_viral_sample
    random.shuffle(all_videos)

    # Split Ð½Ð° train/val/test
    total = len(all_videos)
    test_size = int(total * test_split)
    val_size = int(total * val_split)
    train_size = total - test_size - val_size

    train_data = all_videos[:train_size]
    val_data = all_videos[train_size:train_size + val_size]
    test_data = all_videos[train_size + val_size:]

    print(f"\nðŸ“¦ Dataset splits:")
    print(f"  Train: {len(train_data):,} videos")
    print(f"  Val:   {len(val_data):,} videos")
    print(f"  Test:  {len(test_data):,} videos")

    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    print(f"\nðŸ’¾ Saving to {output_dir}...")

    for split_name, split_data in [("train", train_data), ("val", val_data), ("test", test_data)]:
        df = pd.DataFrame(split_data)

        # CSV Ð´Ð»Ñ ÑƒÐ´Ð¾Ð±ÑÑ‚Ð²Ð° Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð°
        csv_path = output_path / f"{split_name}.csv"
        df.to_csv(csv_path, index=False)

        # JSON Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð² PyTorch/HuggingFace
        json_path = output_path / f"{split_name}.json"
        with open(json_path, 'w') as f:
            json.dump(split_data, f, indent=2)

        print(f"  âœ… {split_name}: {csv_path}")

    # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
    stats = {
        "total_videos": total,
        "viral_count": sum(1 for v in all_videos if v['is_viral']),
        "non_viral_count": sum(1 for v in all_videos if not v['is_viral']),
        "viral_ratio": sum(1 for v in all_videos if v['is_viral']) / total,
        "avg_virality_score": sum(v['virality_score'] for v in all_videos) / total,
        "avg_engagement_rate": sum(v['engagement_rate'] for v in all_videos) / total,
        "splits": {
            "train": len(train_data),
            "val": len(val_data),
            "test": len(test_data)
        }
    }

    stats_path = output_path / "stats.json"
    with open(stats_path, 'w') as f:
        json.dump(stats, f, indent=2)

    print(f"\nðŸ“Š Dataset statistics:")
    print(f"  Viral ratio: {stats['viral_ratio']*100:.1f}%")
    print(f"  Avg virality score: {stats['avg_virality_score']:.1f}")
    print(f"  Avg engagement rate: {stats['avg_engagement_rate']*100:.2f}%")
    print(f"  Stats saved to: {stats_path}")

    print("\nâœ¨ Dataset preparation complete!")
    print(f"ðŸ“ Output directory: {output_dir}")
    print(f"\nNext steps:")
    print(f"  1. Download videos using video IDs from the CSVs")
    print(f"  2. Run fine-tuning script with prepared labels")
    print(f"  3. Evaluate on test set")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Prepare TikTok-10M dataset for viral video classification")
    parser.add_argument("--output-dir", type=str, default="./data/tiktok_viral", help="Output directory")
    parser.add_argument("--num-samples", type=int, default=100_000, help="Number of samples to process")
    parser.add_argument("--viral-ratio", type=float, default=0.3, help="Ratio of viral videos (0-1)")
    parser.add_argument("--test-split", type=float, default=0.1, help="Test set ratio")
    parser.add_argument("--val-split", type=float, default=0.1, help="Validation set ratio")

    args = parser.parse_args()

    prepare_dataset_for_training(
        output_dir=args.output_dir,
        num_samples=args.num_samples,
        viral_ratio=args.viral_ratio,
        test_split=args.test_split,
        val_split=args.val_split
    )
